{"Controls":[{"id":"3","version":"gke-1.8.0","detected_version":"none","text":"Worker Nodes","node_type":"node","tests":[{"section":"3.1","type":"","pass":4,"fail":0,"warn":0,"info":0,"desc":"Worker Node Configuration Files","results":[{"test_number":"3.1.1","test_desc":"Ensure that the kubeconfig file permissions are set to 644 or more restrictive (Automated)","audit":"/bin/sh -c 'if test -e /var/lib/kubelet/kubeconfig; then stat -c permissions=%a /var/lib/kubelet/kubeconfig; fi' ","AuditEnv":"","AuditConfig":"","type":"","remediation":"Run the below command (based on the file location on your system) on each worker node.\nFor example,\n\n  chmod 644 /var/lib/kubelet/kubeconfig\n","test_info":["Run the below command (based on the file location on your system) on each worker node.\nFor example,\n\n  chmod 644 /var/lib/kubelet/kubeconfig\n"],"status":"PASS","actual_value":"permissions=644","scored":true,"IsMultiple":false,"expected_result":"permissions has permissions 644, expected 644 or more restrictive"},{"test_number":"3.1.2","test_desc":"Ensure that the kubelet kubeconfig file ownership is set to root:root (Automated)","audit":"/bin/sh -c 'if test -e /var/lib/kubelet/kubeconfig; then stat -c %U:%G /var/lib/kubelet/kubeconfig; fi' ","AuditEnv":"","AuditConfig":"","type":"","remediation":"Run the below command (based on the file location on your system) on each worker node.\nFor example:\n\n  chown root:root /var/lib/kubelet/kubeconfig\n","test_info":["Run the below command (based on the file location on your system) on each worker node.\nFor example:\n\n  chown root:root /var/lib/kubelet/kubeconfig\n"],"status":"PASS","actual_value":"root:root","scored":true,"IsMultiple":false,"expected_result":"'root:root' is present"},{"test_number":"3.1.3","test_desc":"Ensure that the kubelet configuration file has permissions set to 644 (Automated)","audit":"/bin/sh -c 'if test -e /etc/kubernetes/kubelet-config.yaml; then stat -c permissions=%a /etc/kubernetes/kubelet-config.yaml; fi' ","AuditEnv":"","AuditConfig":"","type":"","remediation":"Run the following command (using the kubelet config file location)\n\n  chmod 644 /etc/kubernetes/kubelet-config.yaml\n","test_info":["Run the following command (using the kubelet config file location)\n\n  chmod 644 /etc/kubernetes/kubelet-config.yaml\n"],"status":"PASS","actual_value":"permissions=644","scored":true,"IsMultiple":false,"expected_result":"permissions has permissions 644, expected 644 or more restrictive"},{"test_number":"3.1.4","test_desc":"Ensure that the kubelet configuration file ownership is set to root:root (Automated)","audit":"/bin/sh -c 'if test -e /etc/kubernetes/kubelet-config.yaml; then stat -c %U:%G /etc/kubernetes/kubelet-config.yaml; fi' ","AuditEnv":"","AuditConfig":"","type":"","remediation":"Run the following command (using the config file location identied in the Audit step)\n\n  chown root:root /etc/kubernetes/kubelet-config.yaml\n","test_info":["Run the following command (using the config file location identied in the Audit step)\n\n  chown root:root /etc/kubernetes/kubelet-config.yaml\n"],"status":"PASS","actual_value":"root:root","scored":true,"IsMultiple":false,"expected_result":"'root:root' is present"}]}],"total_pass":4,"total_fail":0,"total_warn":0,"total_info":0},{"id":"4","version":"gke-1.8.0","detected_version":"none","text":"Kubernetes Policies","node_type":"policies","tests":[{"section":"4.1","type":"","pass":8,"fail":1,"warn":1,"info":0,"desc":"RBAC and Service Accounts","results":[{"test_number":"4.1.1","test_desc":"Ensure that the cluster-admin role is only used where required (Automated)","audit":"kubectl get clusterrolebindings -o json | jq -r '\n  [\n    .items[]\n    | select(.roleRef.name == \"cluster-admin\")\n    | .subjects[]?\n    | select(.kind != \"Group\" or .name != \"system:masters\")\n  ]\n  | if length == 0\n    then \"NO_CLUSTER_ADMIN_BINDINGS\"\n    else \"FOUND_CLUSTER_ADMIN_BINDING\"\n    end\n'\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Identify all ClusterRoleBindings to the \"cluster-admin\" role and review their subjects:\n\n  kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECTS:.subjects[*].name | grep cluster-admin\n\nIf non-system principals (users, groups, or service accounts) do not strictly require cluster-admin,\nrebind them to a least-privileged (Cluster)Role and then remove the excessive binding:\n\n  kubectl delete clusterrolebinding \u003cbinding-name\u003e\n\nNotes:\n- Do not modify bindings with the \"system:\" prefix that are required for core components.\n- Prefer assigning narrowly scoped Roles/ClusterRoles that grant only the permissions needed.\n","test_info":["Identify all ClusterRoleBindings to the \"cluster-admin\" role and review their subjects:\n\n  kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECTS:.subjects[*].name | grep cluster-admin\n\nIf non-system principals (users, groups, or service accounts) do not strictly require cluster-admin,\nrebind them to a least-privileged (Cluster)Role and then remove the excessive binding:\n\n  kubectl delete clusterrolebinding \u003cbinding-name\u003e\n\nNotes:\n- Do not modify bindings with the \"system:\" prefix that are required for core components.\n- Prefer assigning narrowly scoped Roles/ClusterRoles that grant only the permissions needed.\n"],"status":"PASS","actual_value":"Error from server (Forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nNO_CLUSTER_ADMIN_BINDINGS","scored":true,"IsMultiple":false,"expected_result":"'NO_CLUSTER_ADMIN_BINDINGS' is equal to 'NO_CLUSTER_ADMIN_BINDINGS'"},{"test_number":"4.1.2","test_desc":"Minimize access to secrets (Automated)","audit":"count=$(kubectl get roles --all-namespaces -o json | jq '\n  .items[]\n  | select(.rules[]?\n    | (.resources[]? == \"secrets\")\n    and ((.verbs[]? == \"get\") or (.verbs[]? == \"list\") or (.verbs[]? == \"watch\"))\n  )' | wc -l)\n\nif [ \"$count\" -gt 0 ]; then\n  echo \"SECRETS_ACCESS_FOUND\"\nfi\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Where possible, remove get, list and watch access to Secret objects in the cluster.\n","test_info":["Where possible, remove get, list and watch access to Secret objects in the cluster.\n"],"status":"PASS","actual_value":"Error from server (Forbidden): roles.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"roles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope","scored":true,"IsMultiple":false,"expected_result":"'SECRETS_ACCESS_FOUND' is not present"},{"test_number":"4.1.3","test_desc":"Minimize wildcard use in Roles and ClusterRoles (Automated)","audit":"wildcards=$(kubectl get roles --all-namespaces -o json | jq '\n  .items[] | select(\n    .rules[]? | (.verbs[]? == \"*\" or .resources[]? == \"*\" or .apiGroups[]? == \"*\")\n  )' | wc -l)\n\nwildcards_clusterroles=$(kubectl get clusterroles -o json | jq '\n  .items[] | select(\n    .rules[]? | (.verbs[]? == \"*\" or .resources[]? == \"*\" or .apiGroups[]? == \"*\")\n  )' | wc -l)\n\ntotal=$((wildcards + wildcards_clusterroles))\n\nif [ \"$total\" -gt 0 ]; then\n  echo \"wildcards_present\"\nfi\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Where possible replace any use of wildcards in clusterroles and roles with specific\nobjects or actions.\n","test_info":["Where possible replace any use of wildcards in clusterroles and roles with specific\nobjects or actions.\n"],"status":"PASS","actual_value":"Error from server (Forbidden): roles.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"roles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): clusterroles.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope","scored":true,"IsMultiple":false,"expected_result":"'wildcards_present' is not present"},{"test_number":"4.1.4","test_desc":"Ensure that default service accounts are not actively used (Automated)","audit":"echo \"ðŸ”¹ Default Service Accounts with automountServiceAccountToken enabled:\"\ndefault_sa_count=$(kubectl get serviceaccounts --all-namespaces -o json | jq '\n  [.items[] | select(.metadata.name == \"default\" and (.automountServiceAccountToken != false))] | length')\nif [ \"$default_sa_count\" -gt 0 ]; then\n  echo \"default_sa_not_auto_mounted\"\nfi\n\necho \"\\nðŸ”¹ Pods using default ServiceAccount:\"\npods_using_default_sa=$(kubectl get pods --all-namespaces -o json | jq '\n  [.items[] | select(.spec.serviceAccountName == \"default\")] | length')\nif [ \"$pods_using_default_sa\" -gt 0 ]; then\n  echo \"default_sa_used_in_pods\"\nfi\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Create explicit service accounts wherever a Kubernetes workload requires specific\naccess to the Kubernetes API server.\n\nModify the configuration of each default service account to include this value\n\n  automountServiceAccountToken: false\n","test_info":["Create explicit service accounts wherever a Kubernetes workload requires specific\naccess to the Kubernetes API server.\n\nModify the configuration of each default service account to include this value\n\n  automountServiceAccountToken: false\n"],"status":"PASS","actual_value":"ðŸ”¹ Default Service Accounts with automountServiceAccountToken enabled:\nError from server (Forbidden): serviceaccounts is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"serviceaccounts\" in API group \"\" at the cluster scope\n\\nðŸ”¹ Pods using default ServiceAccount:\nError from server (Forbidden): pods is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"pods\" in API group \"\" at the cluster scope","scored":true,"IsMultiple":false,"expected_result":"'default_sa_not_auto_mounted' is not present AND 'default_sa_used_in_pods' is not present"},{"test_number":"4.1.5","test_desc":"Ensure that Service Account Tokens are only mounted where necessary (Automated)","audit":"echo \"ðŸ”¹ Pods with automountServiceAccountToken enabled:\"\npods_with_token_mount=$(kubectl get pods --all-namespaces -o json | jq '\n  [.items[] | select(.spec.automountServiceAccountToken != false)] | length')\n\nif [ \"$pods_with_token_mount\" -gt 0 ]; then\n  echo \"automountServiceAccountToken\"\nfi\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Modify the definition of pods and service accounts which do not need to mount service\naccount tokens to disable it.\n","test_info":["Modify the definition of pods and service accounts which do not need to mount service\naccount tokens to disable it.\n"],"status":"FAIL","actual_value":"ðŸ”¹ Pods with automountServiceAccountToken enabled:\nError from server (Forbidden): pods is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"pods\" in API group \"\" at the cluster scope","scored":true,"IsMultiple":false,"expected_result":"'automountServiceAccountToken' is not present"},{"test_number":"4.1.6","test_desc":"Avoid use of system:masters group (Automated)","audit":"found=0\nfor csr in $(kubectl get csr -o name 2\u003e/dev/null | sed 's|^.*/||'); do\n  req=$(kubectl get csr \"$csr\" -o jsonpath='{.spec.request}' 2\u003e/dev/null)\n  [ -z \"$req\" ] \u0026\u0026 continue\n  if echo \"$req\" | base64 -d 2\u003e/dev/null | openssl req -noout -text 2\u003e/dev/null | grep -q 'O = system:masters'; then\n    conds=$(kubectl get csr \"$csr\" -o json | jq -r '[.status.conditions[]?.type] | join(\",\")')\n    echo \"FOUND_SYSTEM_MASTERS_CSR:${csr}:${conds:-NONE}\"\n    found=1\n  fi\ndone\nif [ \"$found\" -eq 0 ]; then\n  echo \"NO_SYSTEM_MASTERS_CREDENTIALS_FOUND\"\nfi\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Remove the system:masters group from all users in the cluster.\n","test_info":["Remove the system:masters group from all users in the cluster.\n"],"status":"PASS","actual_value":"NO_SYSTEM_MASTERS_CREDENTIALS_FOUND","scored":true,"IsMultiple":false,"expected_result":"'NO_SYSTEM_MASTERS_CREDENTIALS_FOUND' is equal to 'NO_SYSTEM_MASTERS_CREDENTIALS_FOUND'"},{"test_number":"4.1.7","test_desc":"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster (Manual)","audit":"","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Where possible, remove the impersonate, bind and escalate rights from subjects.\n","test_info":["Where possible, remove the impersonate, bind and escalate rights from subjects.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"4.1.8","test_desc":"Avoid bindings to system:anonymous (Automated)","audit":"# Flags any ClusterRoleBinding/RoleBinding that targets the user \"system:anonymous\".\n# Prints \"NO_ANONYMOUS_BINDINGS\" when none are found.\n(\n  kubectl get clusterrolebindings -o json | jq -r '\n    .items[]\n    | select((.subjects | length) \u003e 0)\n    | select(any(.subjects[]?;\n        .kind==\"User\" and .name==\"system:anonymous\"\n      ))\n    | \"FOUND_ANONYMOUS:ClusterRoleBinding:\\(.metadata.name):ROLE=\\(.roleRef.kind)/\\(.roleRef.name)\"\n  ';\n  kubectl get rolebindings -A -o json | jq -r '\n    .items[]\n    | select((.subjects | length) \u003e 0)\n    | select(any(.subjects[]?;\n        .kind==\"User\" and .name==\"system:anonymous\"\n      ))\n    | \"FOUND_ANONYMOUS:RoleBinding:\\(.metadata.namespace):\\(.metadata.name):ROLE=\\(.roleRef.kind)/\\(.roleRef.name)\"\n  '\n) | (grep -q '^FOUND_ANONYMOUS:' \u0026\u0026 cat || echo 'NO_ANONYMOUS_BINDINGS')\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Identify all clusterrolebindings and rolebindings to the user system:anonymous.\nCheck if they are used and review the permissions associated with the binding using the\ncommands in the Audit section above or refer to GKE documentation\n(https://cloud.google.com/kubernetes-engine/docs/best-practices/rbac#detect-prevent-default).\n\nStrongly consider replacing unsafe bindings with an authenticated, user-defined group.\nWhere possible, bind to non-default, user-defined groups with least-privilege roles.\n\nIf there are any unsafe bindings to the user system:anonymous, proceed to delete them\nafter consideration for cluster operations with only necessary, safer bindings.\n\n  kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME]\n  kubectl delete rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE]\n","test_info":["Identify all clusterrolebindings and rolebindings to the user system:anonymous.\nCheck if they are used and review the permissions associated with the binding using the\ncommands in the Audit section above or refer to GKE documentation\n(https://cloud.google.com/kubernetes-engine/docs/best-practices/rbac#detect-prevent-default).\n\nStrongly consider replacing unsafe bindings with an authenticated, user-defined group.\nWhere possible, bind to non-default, user-defined groups with least-privilege roles.\n\nIf there are any unsafe bindings to the user system:anonymous, proceed to delete them\nafter consideration for cluster operations with only necessary, safer bindings.\n\n  kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME]\n  kubectl delete rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE]\n"],"status":"PASS","actual_value":"Error from server (Forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): rolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"rolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nNO_ANONYMOUS_BINDINGS","scored":true,"IsMultiple":false,"expected_result":"'NO_ANONYMOUS_BINDINGS' is equal to 'NO_ANONYMOUS_BINDINGS'"},{"test_number":"4.1.9","test_desc":"Avoid non-default bindings to system:unauthenticated (Automated)","audit":"# Flags any non-default binding to the group \"system:unauthenticated\".\n# Prints \"NO_NON_DEFAULT_UNAUTH_BINDINGS\" when none are found.\n(\n  kubectl get clusterrolebindings -o json | jq -r '\n    .items[]\n    | select(.metadata.name != \"system:public-info-viewer\")\n    | select((.subjects | length) \u003e 0)\n    | select(any(.subjects[]?;\n        .kind==\"Group\" and .name==\"system:unauthenticated\"\n      ))\n    | \"FOUND_UNAUTH:ClusterRoleBinding:\\(.metadata.name):ROLE=\\(.roleRef.kind)/\\(.roleRef.name)\"\n  ';\n  kubectl get rolebindings -A -o json | jq -r '\n    .items[]\n    | select((.subjects | length) \u003e 0)\n    | select(any(.subjects[]?;\n        .kind==\"Group\" and .name==\"system:unauthenticated\"\n      ))\n    | \"FOUND_UNAUTH:RoleBinding:\\(.metadata.namespace):\\(.metadata.name):ROLE=\\(.roleRef.kind)/\\(.roleRef.name)\"\n  '\n) | (grep -q \"^FOUND_UNAUTH:\" \u0026\u0026 cat || echo \"NO_NON_DEFAULT_UNAUTH_BINDINGS\")\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Identify all non-default clusterrolebindings and rolebindings to the group\nsystem:unauthenticated. Check if they are used and review the permissions\nassociated with the binding using the commands in the Audit section above or refer to\nGKE documentation (https://cloud.google.com/kubernetes-engine/docs/best-practices/rbac#detect-prevent-default).\n\nStrongly consider replacing non-default, unsafe bindings with an authenticated, user-\ndefined group. Where possible, bind to non-default, user-defined groups with least-\nprivilege roles.\n\nIf there are any non-default, unsafe bindings to the group system:unauthenticated,\nproceed to delete them after consideration for cluster operations with only necessary,\nsafer bindings.\n\n  kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME]\n  kubectl delete rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE]\n","test_info":["Identify all non-default clusterrolebindings and rolebindings to the group\nsystem:unauthenticated. Check if they are used and review the permissions\nassociated with the binding using the commands in the Audit section above or refer to\nGKE documentation (https://cloud.google.com/kubernetes-engine/docs/best-practices/rbac#detect-prevent-default).\n\nStrongly consider replacing non-default, unsafe bindings with an authenticated, user-\ndefined group. Where possible, bind to non-default, user-defined groups with least-\nprivilege roles.\n\nIf there are any non-default, unsafe bindings to the group system:unauthenticated,\nproceed to delete them after consideration for cluster operations with only necessary,\nsafer bindings.\n\n  kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME]\n  kubectl delete rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE]\n"],"status":"PASS","actual_value":"Error from server (Forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): rolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"rolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nNO_NON_DEFAULT_UNAUTH_BINDINGS","scored":true,"IsMultiple":false,"expected_result":"'NO_NON_DEFAULT_UNAUTH_BINDINGS' is equal to 'NO_NON_DEFAULT_UNAUTH_BINDINGS'"},{"test_number":"4.1.10","test_desc":"Avoid non-default bindings to system:authenticated (Automated)","audit":"# Flags any non-default binding to the group \"system:authenticated\".\n# Allowed defaults (CRB): system:basic-user, system:discovery\n# Prints \"NO_NON_DEFAULT_AUTH_BINDINGS\" when none are found.\n(\n  kubectl get clusterrolebindings -o json | jq -r '\n    .items[]\n    | select((.metadata.name != \"system:basic-user\") and (.metadata.name != \"system:discovery\"))\n    | select((.subjects | length) \u003e 0)\n    | select(any(.subjects[]?;\n        .kind==\"Group\" and .name==\"system:authenticated\"\n      ))\n    | \"FOUND_AUTH:ClusterRoleBinding:\\(.metadata.name):ROLE=\\(.roleRef.kind)/\\(.roleRef.name)\"\n  ';\n  kubectl get rolebindings -A -o json | jq -r '\n    .items[]\n    | select((.subjects | length) \u003e 0)\n    | select(any(.subjects[]?;\n        .kind==\"Group\" and .name==\"system:authenticated\"\n      ))\n    | \"FOUND_AUTH:RoleBinding:\\(.metadata.namespace):\\(.metadata.name):ROLE=\\(.roleRef.kind)/\\(.roleRef.name)\"\n  '\n) | (grep -q \"^FOUND_AUTH:\" \u0026\u0026 cat || echo \"NO_NON_DEFAULT_AUTH_BINDINGS\")\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Identify all non-default clusterrolebindings and rolebindings to the group\nsystem:authenticated. Check if they are used and review the permissions associated\nwith the binding using the commands in the Audit section above or refer to GKE\ndocumentation.\n\nStrongly consider replacing non-default, unsafe bindings with an authenticated, user-\ndefined group. Where possible, bind to non-default, user-defined groups with least-\nprivilege roles.\n\nIf there are any non-default, unsafe bindings to the group system:authenticated,\nproceed to delete them after consideration for cluster operations with only necessary,\nsafer bindings.\n\n  kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME]\n  kubectl delete rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE]\n","test_info":["Identify all non-default clusterrolebindings and rolebindings to the group\nsystem:authenticated. Check if they are used and review the permissions associated\nwith the binding using the commands in the Audit section above or refer to GKE\ndocumentation.\n\nStrongly consider replacing non-default, unsafe bindings with an authenticated, user-\ndefined group. Where possible, bind to non-default, user-defined groups with least-\nprivilege roles.\n\nIf there are any non-default, unsafe bindings to the group system:authenticated,\nproceed to delete them after consideration for cluster operations with only necessary,\nsafer bindings.\n\n  kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME]\n  kubectl delete rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE]\n"],"status":"PASS","actual_value":"Error from server (Forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): rolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"rolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nNO_NON_DEFAULT_AUTH_BINDINGS","scored":true,"IsMultiple":false,"expected_result":"'NO_NON_DEFAULT_AUTH_BINDINGS' is equal to 'NO_NON_DEFAULT_AUTH_BINDINGS'"}]},{"section":"4.2","type":"","pass":0,"fail":0,"warn":1,"info":0,"desc":"Pod Security Standards","results":[{"test_number":"4.2.1","test_desc":"Ensure that the cluster enforces Pod Security Standard Baseline profile or stricter for all namespaces. (Manual)","audit":"","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Ensure that Pod Security Admission is in place for every namespace which contains\nuser workloads.\nRun the following command to enforce the Baseline profile in a namespace:\n\n  kubectl label namespace pod-security.kubernetes.io/enforce=baseline\n","test_info":["Ensure that Pod Security Admission is in place for every namespace which contains\nuser workloads.\nRun the following command to enforce the Baseline profile in a namespace:\n\n  kubectl label namespace pod-security.kubernetes.io/enforce=baseline\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"4.3","type":"","pass":1,"fail":0,"warn":1,"info":0,"desc":"Network Policies and CNI","results":[{"test_number":"4.3.1","test_desc":"Ensure that the CNI in use supports Network Policies (Manual)","audit":"","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"To use a CNI plugin with Network Policy, enable Network Policy in GKE, and the CNI plugin\nwill be updated. See Recommendation 5.6.7.\n","test_info":["To use a CNI plugin with Network Policy, enable Network Policy in GKE, and the CNI plugin\nwill be updated. See Recommendation 5.6.7.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"4.3.2","test_desc":"Ensure that all Namespaces have Network Policies defined (Automated)","audit":"(kubectl get ns -o json; kubectl get networkpolicy -A -o json) \\\n| jq -rs '\n  (.[0].items | map(.metadata.name)\n   | map(select(.!=\"kube-system\" and .!=\"kube-public\" and .!=\"kube-node-lease\"))) as $ns\n  |\n  ( (.[1].items // [])\n    | sort_by(.metadata.namespace)\n    | group_by(.metadata.namespace)\n    | map({key: .[0].metadata.namespace, value: length})\n    | from_entries\n  ) as $np\n  |\n  [ $ns[] | select( ($np[.] // 0) == 0 ) ] as $missing\n  |\n  if ($missing|length)\u003e0\n  then ($missing[] | \"FOUND_NAMESPACE_WITHOUT_NETWORKPOLICY:\"+.)\n  else \"ALL_NAMESPACES_HAVE_NETWORK_POLICIES\"\n  end\n'\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Follow the documentation and create NetworkPolicy objects as needed.\nSee: https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy#creating_a_network_policy\nfor more information.\n","test_info":["Follow the documentation and create NetworkPolicy objects as needed.\nSee: https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy#creating_a_network_policy\nfor more information.\n"],"status":"PASS","actual_value":"Error from server (Forbidden): namespaces is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\nError from server (Forbidden): networkpolicies.networking.k8s.io is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"networkpolicies\" in API group \"networking.k8s.io\" at the cluster scope\nALL_NAMESPACES_HAVE_NETWORK_POLICIES","scored":true,"IsMultiple":false,"expected_result":"'ALL_NAMESPACES_HAVE_NETWORK_POLICIES' is equal to 'ALL_NAMESPACES_HAVE_NETWORK_POLICIES'"}]},{"section":"4.4","type":"","pass":1,"fail":0,"warn":1,"info":0,"desc":"Secrets Management","results":[{"test_number":"4.4.1","test_desc":"Prefer using secrets as files over secrets as environment variables (Automated)","audit":"output=$(kubectl get all --all-namespaces -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}')\nif [ -z \"$output\" ]; then echo \"NO_ENV_SECRET_REFERENCES\"; else echo \"ENV_SECRET_REFERENCES_FOUND\"; fi\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"if possible, rewrite application code to read secrets from mounted secret files, rather than\nfrom environment variables.\n","test_info":["if possible, rewrite application code to read secrets from mounted secret files, rather than\nfrom environment variables.\n"],"status":"PASS","actual_value":"Error from server (Forbidden): pods is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"pods\" in API group \"\" at the cluster scope\nError from server (Forbidden): replicationcontrollers is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope\nError from server (Forbidden): services is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"services\" in API group \"\" at the cluster scope\nError from server (Forbidden): daemonsets.apps is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"daemonsets\" in API group \"apps\" at the cluster scope\nError from server (Forbidden): deployments.apps is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"deployments\" in API group \"apps\" at the cluster scope\nError from server (Forbidden): replicasets.apps is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope\nError from server (Forbidden): statefulsets.apps is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope\nError from server (Forbidden): horizontalpodautoscalers.autoscaling is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"horizontalpodautoscalers\" in API group \"autoscaling\" at the cluster scope\nError from server (Forbidden): cronjobs.batch is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"cronjobs\" in API group \"batch\" at the cluster scope\nError from server (Forbidden): jobs.batch is forbidden: User \"system:serviceaccount:default:default\" cannot list resource \"jobs\" in API group \"batch\" at the cluster scope\nNO_ENV_SECRET_REFERENCES","scored":true,"IsMultiple":false,"expected_result":"'NO_ENV_SECRET_REFERENCES' is equal to 'NO_ENV_SECRET_REFERENCES'"},{"test_number":"4.4.2","test_desc":"Consider external secret storage (Manual)","audit":"","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Refer to the secrets management options offered by your cloud provider or a third-party\nsecrets management solution.\n","test_info":["Refer to the secrets management options offered by your cloud provider or a third-party\nsecrets management solution.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"4.5","type":"","pass":0,"fail":0,"warn":1,"info":0,"desc":"Extensible Admission Control","results":[{"test_number":"4.5.1","test_desc":"Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)","audit":"","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Follow the Kubernetes documentation and setup image provenance.\nAlso see recommendation 5.10.4.\n","test_info":["Follow the Kubernetes documentation and setup image provenance.\nAlso see recommendation 5.10.4.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"4.6","type":"","pass":1,"fail":0,"warn":3,"info":0,"desc":"General Policies","results":[{"test_number":"4.6.1","test_desc":"Create administrative boundaries between resources using namespaces (Manual)","audit":"","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Follow the documentation and create namespaces for objects in your deployment as you need\nthem.\n","test_info":["Follow the documentation and create namespaces for objects in your deployment as you need\nthem.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"4.6.2","test_desc":"Ensure that the seccomp profile is set to RuntimeDefault in your pod definitions (Automated)","audit":"","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Use security context to enable the RuntimeDefault seccomp profile in your pod\ndefinitions. An example is as below:\n\n  {\n    \"namespace\": \"kube-system\",\n    \"name\": \"metrics-server-v0.7.0-dbcc8ddf6-gz7d4\",\n    \"seccompProfile\": \"RuntimeDefault\"\n  }\n","test_info":["Use security context to enable the RuntimeDefault seccomp profile in your pod\ndefinitions. An example is as below:\n\n  {\n    \"namespace\": \"kube-system\",\n    \"name\": \"metrics-server-v0.7.0-dbcc8ddf6-gz7d4\",\n    \"seccompProfile\": \"RuntimeDefault\"\n  }\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"4.6.3","test_desc":"Apply Security Context to Your Pods and Containers (Manual)","audit":"","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Follow the Kubernetes documentation and apply security contexts to your pods. For a\nsuggested list of security contexts, you may refer to the CIS Google Container-\nOptimized OS Benchmark.\n","test_info":["Follow the Kubernetes documentation and apply security contexts to your pods. For a\nsuggested list of security contexts, you may refer to the CIS Google Container-\nOptimized OS Benchmark.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"4.6.4","test_desc":"The default namespace should not be used (Automated)","audit":"output=$(kubectl get all -n default --no-headers 2\u003e/dev/null | grep -v '^service\\s\\+kubernetes\\s' || true)\nif [ -z \"$output\" ]; then echo \"DEFAULT_NAMESPACE_UNUSED\"; else echo \"DEFAULT_NAMESPACE_IN_USE\"; fi\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"Ensure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.\n","test_info":["Ensure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.\n"],"status":"PASS","actual_value":"DEFAULT_NAMESPACE_UNUSED","scored":true,"IsMultiple":false,"expected_result":"'DEFAULT_NAMESPACE_UNUSED' is equal to 'DEFAULT_NAMESPACE_UNUSED'"}]}],"total_pass":11,"total_fail":1,"total_warn":8,"total_info":0},{"id":"5","version":"gke-1.8.0","detected_version":"none","text":"Managed Services","node_type":"managedservices","tests":[{"section":"5.1","type":"","pass":0,"fail":0,"warn":4,"info":0,"desc":"Image Registry and Image Scanning","results":[{"test_number":"5.1.1","test_desc":"Ensure Image Vulnerability Scanning is enabled (Automated)","audit":"gcloud services list --enabled","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"For Images Hosted in GCR:\nUsing Google Cloud Console\nGo to GCR by visiting: https://console.cloud.google.com/gcr\nSelect Settings and, under the Vulnerability Scanning heading, click the TURN ON button.\nUsing Command Line\ngcloud services enable containeranalysis.googleapis.com\nFor Images Hosted in AR:\nUsing Google Cloud Console\nGo to GCR by visiting: https://console.cloud.google.com/artifacts\nSelect Settings and, under the Vulnerability Scanning heading, click the ENABLE button.\nUsing Command Line\ngcloud services enable containerscanning.googleapis.com\n","test_info":["For Images Hosted in GCR:\nUsing Google Cloud Console\nGo to GCR by visiting: https://console.cloud.google.com/gcr\nSelect Settings and, under the Vulnerability Scanning heading, click the TURN ON button.\nUsing Command Line\ngcloud services enable containeranalysis.googleapis.com\nFor Images Hosted in AR:\nUsing Google Cloud Console\nGo to GCR by visiting: https://console.cloud.google.com/artifacts\nSelect Settings and, under the Vulnerability Scanning heading, click the ENABLE button.\nUsing Command Line\ngcloud services enable containerscanning.googleapis.com\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.1.2","test_desc":"Minimize user access to Container Image repositories (Manual)","audit":"gcloud projects get-iam-policy \u003cproject_id\u003e \\\n--flatten=\"bindings[].members\" \\\n--format='table(bindings.members,bindings.role)' \\\n--filter=\"bindings.role:roles/storage.admin OR bindings.role:roles/storage.objectAdmin OR bindings.role:roles/storage.objectCreator OR bindings.role:roles/storage.legacyBucketOwner OR bindings.role:roles/storage.legacyBucketWriter OR bindings.role:roles/storage.legacyObjectOwner\"\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"For Images Hosted in AR:\nUsing Command Line:\n\n  gcloud artifacts repositories set-iam-policy \u003crepository-name\u003e \u003cpath-to-policy-file\u003e \\\n    --location \u003crepository-location\u003e\n\nTo learn how to configure policy files see: https://cloud.google.com/artifact-registry/docs/access-control#grant\n\nFor Images Hosted in GCR:\nUsing Command Line:\nTo change roles at the GCR bucket level:\nFirstly, run the following if read permissions are required:\n\n  gsutil iam ch \u003ctype\u003e:\u003cemail_address\u003e:objectViewer gs://artifacts.\u003cproject_id\u003e.appspot.com\n\nThen remove the excessively privileged role (Storage Admin / Storage Object\nAdmin / Storage Object Creator) using:\n\n  gsutil iam ch -d \u003ctype\u003e:\u003cemail_address\u003e:\u003crole\u003e gs://artifacts.\u003cproject_id\u003e.appspot.com\n\nwhere:\n\u003ctype\u003e can be one of the following:\n  user, if the \u003cemail_address\u003e is a Google account.\n  serviceAccount, if \u003cemail_address\u003e specifies a Service account.\n  \u003cemail_address\u003e can be one of the following:\n    a Google account (for example, someone@example.com).\n    a Cloud IAM service account.\n\nTo modify roles defined at the project level and subsequently inherited within the GCR\nbucket, or the Service Account User role, extract the IAM policy file, modify it\naccordingly and apply it using:\n\n  gcloud projects set-iam-policy \u003cproject_id\u003e \u003cpolicy_file\u003e\n","test_info":["For Images Hosted in AR:\nUsing Command Line:\n\n  gcloud artifacts repositories set-iam-policy \u003crepository-name\u003e \u003cpath-to-policy-file\u003e \\\n    --location \u003crepository-location\u003e\n\nTo learn how to configure policy files see: https://cloud.google.com/artifact-registry/docs/access-control#grant\n\nFor Images Hosted in GCR:\nUsing Command Line:\nTo change roles at the GCR bucket level:\nFirstly, run the following if read permissions are required:\n\n  gsutil iam ch \u003ctype\u003e:\u003cemail_address\u003e:objectViewer gs://artifacts.\u003cproject_id\u003e.appspot.com\n\nThen remove the excessively privileged role (Storage Admin / Storage Object\nAdmin / Storage Object Creator) using:\n\n  gsutil iam ch -d \u003ctype\u003e:\u003cemail_address\u003e:\u003crole\u003e gs://artifacts.\u003cproject_id\u003e.appspot.com\n\nwhere:\n\u003ctype\u003e can be one of the following:\n  user, if the \u003cemail_address\u003e is a Google account.\n  serviceAccount, if \u003cemail_address\u003e specifies a Service account.\n  \u003cemail_address\u003e can be one of the following:\n    a Google account (for example, someone@example.com).\n    a Cloud IAM service account.\n\nTo modify roles defined at the project level and subsequently inherited within the GCR\nbucket, or the Service Account User role, extract the IAM policy file, modify it\naccordingly and apply it using:\n\n  gcloud projects set-iam-policy \u003cproject_id\u003e \u003cpolicy_file\u003e\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.1.3","test_desc":"Minimize cluster access to read-only for Container Image repositories (Manual)","audit":"gcloud projects get-iam-policy \u003cproject_id\u003e \\\n--flatten=\"bindings[].members\" \\\n--format='table(bindings.members,bindings.role)' \\\n--filter=\"bindings.role:roles/storage.admin OR bindings.role:roles/storage.objectAdmin OR bindings.role:roles/storage.objectCreator OR bindings.role:roles/storage.legacyBucketOwner OR bindings.role:roles/storage.legacyBucketWriter OR bindings.role:roles/storage.legacyObjectOwner\"\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"For Images Hosted in AR:\nUsing Command Line:\nAdd artifactregistry.reader role\n\n  gcloud artifacts repositories add-iam-policy-binding \u003crepository\u003e \\\n  --location=\u003crepository-location\u003e \\\n  --member='serviceAccount:\u003cemail-address\u003e' \\\n  --role='roles/artifactregistry.reader'\n\nRemove any roles other than artifactregistry.reader\n\n  gcloud artifacts repositories remove-iam-policy-binding \u003crepository\u003e \\\n  --location \u003crepository-location\u003e \\\n  --member='serviceAccount:\u003cemail-address\u003e' \\\n  --role='\u003crole-name\u003e'\n\nFor Images Hosted in GCR:\nFor an account explicitly granted to the bucket:\nFirstly add read access to the Kubernetes Service Account:\n\n  gsutil iam ch \u003ctype\u003e:\u003cemail_address\u003e:objectViewer gs://artifacts.\u003cproject_id\u003e.appspot.com\n\n  where:\n    \u003ctype\u003e can be one of the following:\n      user, if the \u003cemail_address\u003e is a Google account.\n      serviceAccount, if \u003cemail_address\u003e specifies a Service account.\n      \u003cemail_address\u003e can be one of the following:\n        a Google account (for example, someone@example.com).\n        a Cloud IAM service account.\n\nThen remove the excessively privileged role (Storage Admin / Storage Object\nAdmin / Storage Object Creator) using:\n\n  gsutil iam ch -d \u003ctype\u003e:\u003cemail_address\u003e:\u003crole\u003e gs://artifacts.\u003cproject_id\u003e.appspot.com\n\nFor an account that inherits access to the GCR Bucket through Project level\npermissions, modify the Projects IAM policy file accordingly, then upload it using:\n\n  gcloud projects set-iam-policy \u003cproject_id\u003e \u003cpolicy_file\u003e\n","test_info":["For Images Hosted in AR:\nUsing Command Line:\nAdd artifactregistry.reader role\n\n  gcloud artifacts repositories add-iam-policy-binding \u003crepository\u003e \\\n  --location=\u003crepository-location\u003e \\\n  --member='serviceAccount:\u003cemail-address\u003e' \\\n  --role='roles/artifactregistry.reader'\n\nRemove any roles other than artifactregistry.reader\n\n  gcloud artifacts repositories remove-iam-policy-binding \u003crepository\u003e \\\n  --location \u003crepository-location\u003e \\\n  --member='serviceAccount:\u003cemail-address\u003e' \\\n  --role='\u003crole-name\u003e'\n\nFor Images Hosted in GCR:\nFor an account explicitly granted to the bucket:\nFirstly add read access to the Kubernetes Service Account:\n\n  gsutil iam ch \u003ctype\u003e:\u003cemail_address\u003e:objectViewer gs://artifacts.\u003cproject_id\u003e.appspot.com\n\n  where:\n    \u003ctype\u003e can be one of the following:\n      user, if the \u003cemail_address\u003e is a Google account.\n      serviceAccount, if \u003cemail_address\u003e specifies a Service account.\n      \u003cemail_address\u003e can be one of the following:\n        a Google account (for example, someone@example.com).\n        a Cloud IAM service account.\n\nThen remove the excessively privileged role (Storage Admin / Storage Object\nAdmin / Storage Object Creator) using:\n\n  gsutil iam ch -d \u003ctype\u003e:\u003cemail_address\u003e:\u003crole\u003e gs://artifacts.\u003cproject_id\u003e.appspot.com\n\nFor an account that inherits access to the GCR Bucket through Project level\npermissions, modify the Projects IAM policy file accordingly, then upload it using:\n\n  gcloud projects set-iam-policy \u003cproject_id\u003e \u003cpolicy_file\u003e\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.1.4","test_desc":"Ensure only trusted container images are used (Manual)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq .binaryAuthorization\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nUpdate the cluster to enable Binary Authorization:\n\n  gcloud container cluster update \u003ccluster_name\u003e --enable-binauthz\n\nCreate a Binary Authorization Policy using the Binary Authorization Policy Reference:\nhttps://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance.\n\nImport the policy file into Binary Authorization:\n\n  gcloud container binauthz policy import \u003cyaml_policy\u003e\n","test_info":["Using Command Line:\nUpdate the cluster to enable Binary Authorization:\n\n  gcloud container cluster update \u003ccluster_name\u003e --enable-binauthz\n\nCreate a Binary Authorization Policy using the Binary Authorization Policy Reference:\nhttps://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance.\n\nImport the policy file into Binary Authorization:\n\n  gcloud container binauthz policy import \u003cyaml_policy\u003e\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"5.2","type":"","pass":0,"fail":0,"warn":2,"info":0,"desc":"Identity and Access Management (IAM)","results":[{"test_number":"5.2.1","test_desc":"Ensure GKE clusters are not running using the Compute Engine default service account (Automated))","audit":"gcloud container node-pools describe $NODE_POOL --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.config.serviceAccount'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo create a minimally privileged service account:\n\n  gcloud iam service-accounts create \u003cnode_sa_name\u003e \\\n    --display-name \"GKE Node Service Account\"\n  export NODE_SA_EMAIL=gcloud iam service-accounts list \\\n    --format='value(email)' --filter='displayName:GKE Node Service Account'\n\nGrant the following roles to the service account:\n\n  export PROJECT_ID=gcloud config get-value project\n  gcloud projects add-iam-policy-binding \u003cproject_id\u003e --member \\\n    serviceAccount:\u003cnode_sa_email\u003e --role roles/monitoring.metricWriter\n  gcloud projects add-iam-policy-binding \u003cproject_id\u003e --member \\\n    serviceAccount:\u003cnode_sa_email\u003e --role roles/monitoring.viewer\n  gcloud projects add-iam-policy-binding \u003cproject_id\u003e --member \\\n    serviceAccount:\u003cnode_sa_email\u003e --role roles/logging.logWriter\n\nTo create a new Node pool using the Service account, run the following command:\n\n  gcloud container node-pools create \u003cnode_pool\u003e \\\n    --service-account=\u003csa_name\u003e@\u003cproject_id\u003e.iam.gserviceaccount.com \\\n    --cluster=\u003ccluster_name\u003e --zone \u003ccompute_zone\u003e\n\nNote: The workloads will need to be migrated to the new Node pool, and the old node\npools that use the default service account should be deleted to complete the\nremediation.\n","test_info":["Using Command Line:\nTo create a minimally privileged service account:\n\n  gcloud iam service-accounts create \u003cnode_sa_name\u003e \\\n    --display-name \"GKE Node Service Account\"\n  export NODE_SA_EMAIL=gcloud iam service-accounts list \\\n    --format='value(email)' --filter='displayName:GKE Node Service Account'\n\nGrant the following roles to the service account:\n\n  export PROJECT_ID=gcloud config get-value project\n  gcloud projects add-iam-policy-binding \u003cproject_id\u003e --member \\\n    serviceAccount:\u003cnode_sa_email\u003e --role roles/monitoring.metricWriter\n  gcloud projects add-iam-policy-binding \u003cproject_id\u003e --member \\\n    serviceAccount:\u003cnode_sa_email\u003e --role roles/monitoring.viewer\n  gcloud projects add-iam-policy-binding \u003cproject_id\u003e --member \\\n    serviceAccount:\u003cnode_sa_email\u003e --role roles/logging.logWriter\n\nTo create a new Node pool using the Service account, run the following command:\n\n  gcloud container node-pools create \u003cnode_pool\u003e \\\n    --service-account=\u003csa_name\u003e@\u003cproject_id\u003e.iam.gserviceaccount.com \\\n    --cluster=\u003ccluster_name\u003e --zone \u003ccompute_zone\u003e\n\nNote: The workloads will need to be migrated to the new Node pool, and the old node\npools that use the default service account should be deleted to complete the\nremediation.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.2.2","test_desc":"Prefer using dedicated GCP Service Accounts and Workload Identity (Manual)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq .workloadIdentityConfig\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\n\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccluster_zone\u003e \\\n  --workload-pool \u003cproject_id\u003e.svc.id.goog\n\nNote that existing Node pools are unaffected. New Node pools default to --workload-\nmetadata-from-node=GKE_METADATA_SERVER.\n\nThen, modify existing Node pools to enable GKE_METADATA_SERVER:\n\n  gcloud container node-pools update \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccluster_zone\u003e --workload-metadata=GKE_METADATA\n\nWorkloads may need to be modified in order for them to use Workload Identity as\ndescribed within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.\nAlso consider the effects on the availability of hosted workloads as Node pools\nare updated. It may be more appropriate to create new Node Pools.\n","test_info":["Using Command Line:\n\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccluster_zone\u003e \\\n  --workload-pool \u003cproject_id\u003e.svc.id.goog\n\nNote that existing Node pools are unaffected. New Node pools default to --workload-\nmetadata-from-node=GKE_METADATA_SERVER.\n\nThen, modify existing Node pools to enable GKE_METADATA_SERVER:\n\n  gcloud container node-pools update \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccluster_zone\u003e --workload-metadata=GKE_METADATA\n\nWorkloads may need to be modified in order for them to use Workload Identity as\ndescribed within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.\nAlso consider the effects on the availability of hosted workloads as Node pools\nare updated. It may be more appropriate to create new Node Pools.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"5.3","type":"","pass":0,"fail":0,"warn":1,"info":0,"desc":"Cloud Key Management Service (Cloud KMS)","results":[{"test_number":"5.3.1","test_desc":"Ensure Kubernetes Secrets are encrypted using keys managed in Cloud KMS (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.databaseEncryption'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"To create a key:\nCreate a key ring:\n\n  gcloud kms keyrings create \u003cring_name\u003e --location \u003clocation\u003e --project \\\n    \u003ckey_project_id\u003e\n\nCreate a key:\n\n  gcloud kms keys create \u003ckey_name\u003e --location \u003clocation\u003e --keyring \u003cring_name\u003e \\\n    --purpose encryption --project \u003ckey_project_id\u003e\n\nGrant the Kubernetes Engine Service Agent service account the Cloud KMS\nCryptoKey Encrypter/Decrypter role:\n\n  gcloud kms keys add-iam-policy-binding \u003ckey_name\u003e --location \u003clocation\u003e \\\n  --keyring \u003cring_name\u003e --member serviceAccount:\u003cservice_account_name\u003e \\\n  --role roles/cloudkms.cryptoKeyEncrypterDecrypter --project \u003ckey_project_id\u003e\n\nTo create a new cluster with Application-layer Secrets Encryption:\n\n  gcloud container clusters create \u003ccluster_name\u003e --cluster-version=latest \\\n  --zone \u003czone\u003e \\\n  --database-encryption-key projects/\u003ckey_project_id\u003e/locations/\u003clocation\u003e/keyRings/\u003cring_name\u003e/cryptoKeys/\u003ckey_name\u003e \\\n  --project \u003ccluster_project_id\u003e\n\nTo enable on an existing cluster:\n\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003czone\u003e \\\n  --database-encryption-key projects/\u003ckey_project_id\u003e/locations/\u003clocation\u003e/keyRings/\u003cring_name\u003e/cryptoKeys/\u003ckey_name\u003e \\\n  --project \u003ccluster_project_id\u003e\n","test_info":["To create a key:\nCreate a key ring:\n\n  gcloud kms keyrings create \u003cring_name\u003e --location \u003clocation\u003e --project \\\n    \u003ckey_project_id\u003e\n\nCreate a key:\n\n  gcloud kms keys create \u003ckey_name\u003e --location \u003clocation\u003e --keyring \u003cring_name\u003e \\\n    --purpose encryption --project \u003ckey_project_id\u003e\n\nGrant the Kubernetes Engine Service Agent service account the Cloud KMS\nCryptoKey Encrypter/Decrypter role:\n\n  gcloud kms keys add-iam-policy-binding \u003ckey_name\u003e --location \u003clocation\u003e \\\n  --keyring \u003cring_name\u003e --member serviceAccount:\u003cservice_account_name\u003e \\\n  --role roles/cloudkms.cryptoKeyEncrypterDecrypter --project \u003ckey_project_id\u003e\n\nTo create a new cluster with Application-layer Secrets Encryption:\n\n  gcloud container clusters create \u003ccluster_name\u003e --cluster-version=latest \\\n  --zone \u003czone\u003e \\\n  --database-encryption-key projects/\u003ckey_project_id\u003e/locations/\u003clocation\u003e/keyRings/\u003cring_name\u003e/cryptoKeys/\u003ckey_name\u003e \\\n  --project \u003ccluster_project_id\u003e\n\nTo enable on an existing cluster:\n\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003czone\u003e \\\n  --database-encryption-key projects/\u003ckey_project_id\u003e/locations/\u003clocation\u003e/keyRings/\u003cring_name\u003e/cryptoKeys/\u003ckey_name\u003e \\\n  --project \u003ccluster_project_id\u003e\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"5.4","type":"","pass":0,"fail":0,"warn":1,"info":0,"desc":"Node Metadata","results":[{"test_number":"5.4.1","test_desc":"Ensure the GKE Metadata Server is Enabled (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.nodePools[].config.workloadMetadataConfig'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\n\n  gcloud container clusters update \u003ccluster_name\u003e --identity-namespace=\u003cproject_id\u003e.svc.id.goog\n\nNote that existing Node pools are unaffected. New Node pools default to --workload-\nmetadata-from-node=GKE_METADATA_SERVER.\n\nTo modify an existing Node pool to enable GKE Metadata Server:\n\n  gcloud container node-pools update \u003cnode_pool_name\u003e --cluster=\u003ccluster_name\u003e \\\n    --workload-metadata-from-node=GKE_METADATA_SERVER\n\nWorkloads may need modification in order for them to use Workload Identity as\ndescribed within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.\n","test_info":["Using Command Line:\n\n  gcloud container clusters update \u003ccluster_name\u003e --identity-namespace=\u003cproject_id\u003e.svc.id.goog\n\nNote that existing Node pools are unaffected. New Node pools default to --workload-\nmetadata-from-node=GKE_METADATA_SERVER.\n\nTo modify an existing Node pool to enable GKE Metadata Server:\n\n  gcloud container node-pools update \u003cnode_pool_name\u003e --cluster=\u003ccluster_name\u003e \\\n    --workload-metadata-from-node=GKE_METADATA_SERVER\n\nWorkloads may need modification in order for them to use Workload Identity as\ndescribed within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"5.5","type":"","pass":0,"fail":0,"warn":7,"info":0,"desc":"Node Configuration and Maintenance","results":[{"test_number":"5.5.1","test_desc":"Ensure Container-Optimized OS (cos_containerd) is used for GKE node images (Automated)","audit":"gcloud container node-pools describe $NODE_POOL --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.config.imageType'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo set the node image to cos for an existing cluster's Node pool:\n\n  gcloud container clusters upgrade \u003ccluster_name\u003e --image-type cos_containerd \\\n    --zone \u003ccompute_zone\u003e --node-pool \u003cnode_pool_name\u003e\n","test_info":["Using Command Line:\nTo set the node image to cos for an existing cluster's Node pool:\n\n  gcloud container clusters upgrade \u003ccluster_name\u003e --image-type cos_containerd \\\n    --zone \u003ccompute_zone\u003e --node-pool \u003cnode_pool_name\u003e\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.5.2","test_desc":"Ensure Node Auto-Repair is enabled for GKE nodes (Automated)","audit":"gcloud container node-pools describe $POOL_NAME --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.management'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo enable node auto-repair for an existing cluster's Node pool:\n\n  gcloud container node-pools update \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccompute_zone\u003e --enable-autorepair\n","test_info":["Using Command Line:\nTo enable node auto-repair for an existing cluster's Node pool:\n\n  gcloud container node-pools update \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccompute_zone\u003e --enable-autorepair\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.5.3","test_desc":"Ensure Node Auto-Upgrade is enabled for GKE nodes (Automated)","audit":"gcloud container node-pools describe $POOL_NAME --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.management'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo enable node auto-upgrade for an existing cluster's Node pool, run the following\ncommand:\n\n  gcloud container node-pools update \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccluster_zone\u003e --enable-autoupgrade\n","test_info":["Using Command Line:\nTo enable node auto-upgrade for an existing cluster's Node pool, run the following\ncommand:\n\n  gcloud container node-pools update \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccluster_zone\u003e --enable-autoupgrade\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.5.4","test_desc":"When creating New Clusters - Automate GKE version management using Release Channels (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq .releaseChannel.channel\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nCreate a new cluster by running the following command:\n\n  gcloud container clusters create \u003ccluster_name\u003e --zone \u003ccluster_zone\u003e \\\n    --release-channel \u003crelease_channel\u003e\n\nwhere \u003crelease_channel\u003e is stable or regular, according to requirements.\n","test_info":["Using Command Line:\nCreate a new cluster by running the following command:\n\n  gcloud container clusters create \u003ccluster_name\u003e --zone \u003ccluster_zone\u003e \\\n    --release-channel \u003crelease_channel\u003e\n\nwhere \u003crelease_channel\u003e is stable or regular, according to requirements.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.5.5","test_desc":"Ensure Shielded GKE Nodes are Enabled (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.shieldedNodes'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo migrate an existing cluster, the flag --enable-shielded-nodes needs to be\nspecified in the cluster update command:\n\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccluster_zone\u003e \\\n    --enable-shielded-nodes\n","test_info":["Using Command Line:\nTo migrate an existing cluster, the flag --enable-shielded-nodes needs to be\nspecified in the cluster update command:\n\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccluster_zone\u003e \\\n    --enable-shielded-nodes\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.5.6","test_desc":"Ensure Integrity Monitoring for Shielded GKE Nodes is Enabled (Automated)","audit":"gcloud container node-pools describe $POOL_NAME --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq .config.shieldedInstanceConfig\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo create a Node pool within the cluster with Integrity Monitoring enabled, run the\nfollowing command:\n\n  gcloud container node-pools create \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccompute_zone\u003e --shielded-integrity-monitoring\n\nWorkloads from existing non-conforming Node pools will need to be migrated to the\nnewly created Node pool, then delete non-conforming Node pools to complete the\nremediation\n","test_info":["Using Command Line:\nTo create a Node pool within the cluster with Integrity Monitoring enabled, run the\nfollowing command:\n\n  gcloud container node-pools create \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccompute_zone\u003e --shielded-integrity-monitoring\n\nWorkloads from existing non-conforming Node pools will need to be migrated to the\nnewly created Node pool, then delete non-conforming Node pools to complete the\nremediation\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.5.7","test_desc":"Ensure Secure Boot for Shielded GKE Nodes is Enabled (Automated)","audit":"gcloud container node-pools describe $POOL_NAME --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq .config.shieldedInstanceConfig\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo create a Node pool within the cluster with Secure Boot enabled, run the following\ncommand:\n\n  gcloud container node-pools create \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccompute_zone\u003e --shielded-secure-boot\n\nWorkloads will need to be migrated from existing non-conforming Node pools to the\nnewly created Node pool, then delete the non-conforming pools.\n","test_info":["Using Command Line:\nTo create a Node pool within the cluster with Secure Boot enabled, run the following\ncommand:\n\n  gcloud container node-pools create \u003cnode_pool_name\u003e --cluster \u003ccluster_name\u003e \\\n    --zone \u003ccompute_zone\u003e --shielded-secure-boot\n\nWorkloads will need to be migrated from existing non-conforming Node pools to the\nnewly created Node pool, then delete the non-conforming pools.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"5.6","type":"","pass":0,"fail":1,"warn":6,"info":0,"desc":"Cluster Networking","results":[{"test_number":"5.6.1","test_desc":"Enable VPC Flow Logs and Intranode Visibility (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.networkConfig.enableIntraNodeVisibility'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\n1. Find the subnetwork name associated with the cluster.\n\n  gcloud container clusters describe \u003ccluster_name\u003e \\\n    --region \u003ccluster_region\u003e - -format json | jq '.subnetwork'\n\n2. Update the subnetwork to enable VPC Flow Logs.\n  gcloud compute networks subnets update \u003csubnet_name\u003e --enable-flow-logs\n","test_info":["Using Command Line:\n1. Find the subnetwork name associated with the cluster.\n\n  gcloud container clusters describe \u003ccluster_name\u003e \\\n    --region \u003ccluster_region\u003e - -format json | jq '.subnetwork'\n\n2. Update the subnetwork to enable VPC Flow Logs.\n  gcloud compute networks subnets update \u003csubnet_name\u003e --enable-flow-logs\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.6.2","test_desc":"Ensure use of VPC-native clusters (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.ipAllocationPolicy.useIpAliases'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo enable Alias IP on a new cluster, run the following command:\n\n  gcloud container clusters create \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n    --enable-ip-alias\n","test_info":["Using Command Line:\nTo enable Alias IP on a new cluster, run the following command:\n\n  gcloud container clusters create \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n    --enable-ip-alias\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.6.3","test_desc":"Ensure Control Plane Authorized Networks is Enabled (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.masterAuthorizedNetworksConfig'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo enable Control Plane Authorized Networks for an existing cluster, run the following\ncommand:\n\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n    --enable-master-authorized-networks\n\nAlong with this, you can list authorized networks using the --master-authorized-networks\nflag which contains a list of up to 20 external networks that are allowed to\nconnect to your cluster's control plane through HTTPS. You provide these networks as\na comma-separated list of addresses in CIDR notation (such as 90.90.100.0/24).\n","test_info":["Using Command Line:\nTo enable Control Plane Authorized Networks for an existing cluster, run the following\ncommand:\n\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n    --enable-master-authorized-networks\n\nAlong with this, you can list authorized networks using the --master-authorized-networks\nflag which contains a list of up to 20 external networks that are allowed to\nconnect to your cluster's control plane through HTTPS. You provide these networks as\na comma-separated list of addresses in CIDR notation (such as 90.90.100.0/24).\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.6.4","test_desc":"Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.privateClusterConfig.enablePrivateEndpoint'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nCreate a cluster with a Private Endpoint enabled and Public Access disabled by including\nthe --enable-private-endpoint flag within the cluster create command:\n\n  gcloud container clusters create \u003ccluster_name\u003e --enable-private-endpoint\n\nSetting this flag also requires the setting of --enable-private-nodes, --enable-ip-alias\nand --master-ipv4-cidr=\u003cmaster_cidr_range\u003e.\n","test_info":["Using Command Line:\nCreate a cluster with a Private Endpoint enabled and Public Access disabled by including\nthe --enable-private-endpoint flag within the cluster create command:\n\n  gcloud container clusters create \u003ccluster_name\u003e --enable-private-endpoint\n\nSetting this flag also requires the setting of --enable-private-nodes, --enable-ip-alias\nand --master-ipv4-cidr=\u003cmaster_cidr_range\u003e.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.6.5","test_desc":"Ensure clusters are created with Private Nodes (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.privateClusterConfig.enablePrivateNodes'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo create a cluster with Private Nodes enabled, include the --enable-private-nodes\nflag within the cluster create command:\n\n  gcloud container clusters create \u003ccluster_name\u003e --enable-private-nodes\n\nSetting this flag also requires the setting of --enable-ip-alias and\n--master-ipv4-cidr=\u003cmaster_cidr_range\u003e.\n","test_info":["Using Command Line:\nTo create a cluster with Private Nodes enabled, include the --enable-private-nodes\nflag within the cluster create command:\n\n  gcloud container clusters create \u003ccluster_name\u003e --enable-private-nodes\n\nSetting this flag also requires the setting of --enable-ip-alias and\n--master-ipv4-cidr=\u003cmaster_cidr_range\u003e.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.6.6","test_desc":"Consider firewalling GKE worker nodes (Manual)","audit":"gcloud compute instances describe $INSTANCE_NAME --zone $COMPUTE_ZONE --format json | jq '{tags: .tags.items[], serviceaccount:.serviceAccounts[].email, network: .networkInterfaces[].network}'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nUse the following command to generate firewall rules, setting the variables as\nappropriate:\n\n  gcloud compute firewall-rules create \u003cfirewall_rule_name\u003e \\\n  --network \u003cnetwork\u003e --priority \u003cpriority\u003e --direction \u003cdirection\u003e \\\n  --action \u003caction\u003e --target-tags \u003ctag\u003e \\\n  --target-service-accounts \u003cservice_account\u003e \\\n  --source-ranges \u003csource_cidr_range\u003e --source-tags \u003csource_tags\u003e \\\n  --source-service-accounts \u003csource_service_account\u003e \\\n  --destination-ranges \u003cdestination_cidr_range\u003e --rules \u003crules\u003e\n","test_info":["Using Command Line:\nUse the following command to generate firewall rules, setting the variables as\nappropriate:\n\n  gcloud compute firewall-rules create \u003cfirewall_rule_name\u003e \\\n  --network \u003cnetwork\u003e --priority \u003cpriority\u003e --direction \u003cdirection\u003e \\\n  --action \u003caction\u003e --target-tags \u003ctag\u003e \\\n  --target-service-accounts \u003cservice_account\u003e \\\n  --source-ranges \u003csource_cidr_range\u003e --source-tags \u003csource_tags\u003e \\\n  --source-service-accounts \u003csource_service_account\u003e \\\n  --destination-ranges \u003cdestination_cidr_range\u003e --rules \u003crules\u003e\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.6.7","test_desc":"Ensure use of Google-managed SSL Certificates (Automated)","audit":"svc_json=\"$(kubectl get svc -A -o json 2\u003e/dev/null                || echo '{\"items\":[],\"__err\":\"SVC_FORBIDDEN\"}')\"\ning_json=\"$(kubectl get ingress -A -o json 2\u003e/dev/null            || echo '{\"items\":[],\"__err\":\"INGRESS_FORBIDDEN\"}')\"\nmc_json =\"$(kubectl get managedcertificates -A -o json 2\u003e/dev/null || echo '{\"items\":[],\"__err\":\"MC_FORBIDDEN\"}')\"\n\nprintf '%s\\n%s\\n%s\\n' \"$svc_json\" \"$ing_json\" \"$mc_json\" \\\n| jq -rs '\n  (.[0] // {}) as $svcsRaw |\n  (.[1] // {}) as $ingsRaw |\n  (.[2] // {}) as $mcsRaw |\n\n  # If any list failed, surface an error and DO NOT print the success string\n  if ($svcsRaw.__err or $ingsRaw.__err or $mcsRaw.__err) then\n    \"ERROR_KUBECTL_LIST:\" +\n    ([\n      ($svcsRaw.__err // empty),\n      ($ingsRaw.__err // empty),\n      ($mcsRaw.__err // empty)\n    ] | join(\",\"))\n  else\n    ($svcsRaw.items // []) as $svcs |\n    ($ingsRaw.items // []) as $ings |\n    ($mcsRaw.items // []) as $mcs |\n\n    def trim: gsub(\"^\\\\s+|\\\\s+$\";\"\");\n    def hasmc($ns;$name): any($mcs[]?; .metadata.namespace==$ns and .metadata.name==$name);\n\n    ([\n      # Public Services (not eligible for managed certs)\n      $svcs[]? | select(.spec.type==\"LoadBalancer\")\n      | \"FOUND_PUBLIC_LB_SERVICE:\\(.metadata.namespace // \"default\"):\\(.metadata.name)\"\n    ] + [\n      # Ingresses missing managed-certs annotation\n      $ings[]? as $i\n      | ($i.metadata.annotations.\"networking.gke.io/managed-certificates\" // \"\") as $ann\n      | select($ann==\"\")\n      | \"FOUND_INGRESS_WITHOUT_MANAGED_CERT:\\($i.metadata.namespace // \"default\"):\\($i.metadata.name)\"\n    ] + [\n      # Ingresses referencing non-existent ManagedCertificate(s)\n      $ings[]? as $i\n      | ($i.metadata.annotations.\"networking.gke.io/managed-certificates\" // \"\") as $ann\n      | select($ann!=\"\")\n      | ($i.metadata.namespace // \"default\") as $ns\n      | ($ann | split(\",\") | map(trim) | map(select(length\u003e0)) | .[]) as $mc\n      | select(hasmc($ns;$mc) | not)\n      | \"FOUND_MISSING_MANAGED_CERT_RESOURCE:\\($ns):\\($i.metadata.name):cert=\\($mc)\"\n    ]) as $f\n    | if ($f|length)\u003e0\n        then $f[]\n        else \"ALL_INGRESSES_USE_MANAGED_CERTS_AND_NO_PUBLIC_LB_SERVICES\"\n      end\n  end\n'\n","AuditEnv":"","AuditConfig":"","type":"","remediation":"If services of type:LoadBalancer are discovered, consider replacing the Service with\nan Ingress.\n\nTo configure the Ingress and use Google-managed SSL certificates, follow the\ninstructions as listed at: https://cloud.google.com/kubernetes-engine/docs/how-\nto/managed-certs.\n","test_info":["If services of type:LoadBalancer are discovered, consider replacing the Service with\nan Ingress.\n\nTo configure the Ingress and use Google-managed SSL certificates, follow the\ninstructions as listed at: https://cloud.google.com/kubernetes-engine/docs/how-\nto/managed-certs.\n"],"status":"FAIL","actual_value":"/bin/sh: mc_json: not found\nERROR_KUBECTL_LIST:SVC_FORBIDDEN","scored":true,"IsMultiple":false,"expected_result":"'ALL_INGRESSES_USE_MANAGED_CERTS_AND_NO_PUBLIC_LB_SERVICES' is present"}]},{"section":"5.7","type":"","pass":0,"fail":0,"warn":2,"info":0,"desc":"Logging","results":[{"test_number":"5.7.1","test_desc":"Ensure Logging and Cloud Monitoring is Enabled (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.loggingService'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"To enable Logging for an existing cluster, run the following command:\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n  --logging=\u003ccomponents_to_be_logged\u003e\n\nSee https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--logging\nfor a list of available components for logging.\n\nTo enable Cloud Monitoring for an existing cluster, run the following command:\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n  --monitoring=\u003ccomponents_to_be_logged\u003e\n\nSee https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--\nmonitoring for a list of available components for Cloud Monitoring.\n","test_info":["To enable Logging for an existing cluster, run the following command:\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n  --logging=\u003ccomponents_to_be_logged\u003e\n\nSee https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--logging\nfor a list of available components for logging.\n\nTo enable Cloud Monitoring for an existing cluster, run the following command:\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n  --monitoring=\u003ccomponents_to_be_logged\u003e\n\nSee https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--\nmonitoring for a list of available components for Cloud Monitoring.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.7.2","test_desc":"Enable Linux auditd logging (Manual)","audit":"kubectl get daemonsets -A -o json | jq '.items[] | select (.spec.template.spec.containers[].image | contains (\"gcr.io/stackdriver-agents/stackdriver-logging-agent\"))'| jq '{name: .metadata.name, annotations: .metadata.annotations.\"kubernetes.io/description\", namespace: .metadata.namespace, status: .status}'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nDownload the example manifests:\n  curl https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-node-tools/master/os-audit/cos-auditd-logging.yaml \u003e cos-auditd-logging.yaml\n\nEdit the example manifests if needed. Then, deploy them:\n  kubectl apply -f cos-auditd-logging.yaml\n\nVerify that the logging Pods have started. If a different Namespace was defined in the\nmanifests, replace cos-auditd with the name of the namespace being used:\n  kubectl get pods --namespace=cos-auditd\n","test_info":["Using Command Line:\nDownload the example manifests:\n  curl https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-node-tools/master/os-audit/cos-auditd-logging.yaml \u003e cos-auditd-logging.yaml\n\nEdit the example manifests if needed. Then, deploy them:\n  kubectl apply -f cos-auditd-logging.yaml\n\nVerify that the logging Pods have started. If a different Namespace was defined in the\nmanifests, replace cos-auditd with the name of the namespace being used:\n  kubectl get pods --namespace=cos-auditd\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"5.8","type":"","pass":0,"fail":0,"warn":3,"info":0,"desc":"Authentication and Authorization","results":[{"test_number":"5.8.1","test_desc":"Ensure authentication using Client Certificates is Disabled (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.masterAuth.clientKey'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nCreate a new cluster without a Client Certificate:\n  gcloud container clusters create [CLUSTER_NAME] \\\n    --no-issue-client-certificate\n","test_info":["Using Command Line:\nCreate a new cluster without a Client Certificate:\n  gcloud container clusters create [CLUSTER_NAME] \\\n    --no-issue-client-certificate\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.8.2","test_desc":"Manage Kubernetes RBAC users with Google Groups for GKE (Manual)","audit":"gcloud container clusters create \u003ccluster_name\u003e --security-group \u003csecurity_group_name\u003e\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nFollow the G Suite Groups instructions at: https://cloud.google.com/kubernetes-\nengine/docs/how-to/role-based-access-control#google-groups-for-gke.\n\nThen, create a cluster with:\n  gcloud container clusters create \u003ccluster_name\u003e --security-group \u003csecurity_group_name\u003e\n\nFinally create Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings that\nreference the G Suite Groups.\n","test_info":["Using Command Line:\nFollow the G Suite Groups instructions at: https://cloud.google.com/kubernetes-\nengine/docs/how-to/role-based-access-control#google-groups-for-gke.\n\nThen, create a cluster with:\n  gcloud container clusters create \u003ccluster_name\u003e --security-group \u003csecurity_group_name\u003e\n\nFinally create Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings that\nreference the G Suite Groups.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.8.3","test_desc":"Ensure Legacy Authorization (ABAC) is Disabled (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.legacyAbac'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo disable Legacy Authorization for an existing cluster, run the following command:\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n    --no-enable-legacy-authorization\n","test_info":["Using Command Line:\nTo disable Legacy Authorization for an existing cluster, run the following command:\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003ccompute_zone\u003e \\\n    --no-enable-legacy-authorization\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"5.9","type":"","pass":0,"fail":0,"warn":2,"info":0,"desc":"Storage","results":[{"test_number":"5.9.1","test_desc":"Enable Customer-Managed Encryption Keys (CMEK) for GKE Persistent Disks (PD) (Manual)","audit":"gcloud compute disks describe $PV_NAME --zone $COMPUTE_ZONE --format json | jq '.diskEncryptionKey.kmsKeyName'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nFollow the instructions detailed at: https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek.\n","test_info":["Using Command Line:\nFollow the instructions detailed at: https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.9.2","test_desc":"Enable Customer-Managed Encryption Keys (CMEK) for Boot Disks (Automated)","audit":"gcloud container node-pools describe $NODE_POOL --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nCreate a new node pool using customer-managed encryption keys for the node boot\ndisk, of \u003cdisk_type\u003e either pd-standard or pd-ssd:\n  gcloud container node-pools create \u003ccluster_name\u003e --disk-type \u003cdisk_type\u003e \\\n  --boot-disk-kms-key projects/\u003ckey_project_id\u003e/locations/\u003clocation\u003e/keyRings/\u003cring_name\u003e/cryptoKeys/\u003ckey_name\u003e\n\nCreate a cluster using customer-managed encryption keys for the node boot disk, of\n\u003cdisk_type\u003e either pd-standard or pd-ssd:\n  gcloud container clusters create \u003ccluster_name\u003e --disk-type \u003cdisk_type\u003e \\\n  --boot-disk-kms-key projects/\u003ckey_project_id\u003e/locations/\u003clocation\u003e/keyRings/\u003cring_name\u003e/cryptoKeys/\u003ckey_name\u003e\n","test_info":["Using Command Line:\nCreate a new node pool using customer-managed encryption keys for the node boot\ndisk, of \u003cdisk_type\u003e either pd-standard or pd-ssd:\n  gcloud container node-pools create \u003ccluster_name\u003e --disk-type \u003cdisk_type\u003e \\\n  --boot-disk-kms-key projects/\u003ckey_project_id\u003e/locations/\u003clocation\u003e/keyRings/\u003cring_name\u003e/cryptoKeys/\u003ckey_name\u003e\n\nCreate a cluster using customer-managed encryption keys for the node boot disk, of\n\u003cdisk_type\u003e either pd-standard or pd-ssd:\n  gcloud container clusters create \u003ccluster_name\u003e --disk-type \u003cdisk_type\u003e \\\n  --boot-disk-kms-key projects/\u003ckey_project_id\u003e/locations/\u003clocation\u003e/keyRings/\u003cring_name\u003e/cryptoKeys/\u003ckey_name\u003e\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]},{"section":"5.10","type":"","pass":0,"fail":0,"warn":4,"info":0,"desc":"Other Cluster Configurations","results":[{"test_number":"5.10.1","test_desc":"Ensure Kubernetes Web UI is Disabled (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE  --format json | jq '.addonsConfig.kubernetesDashboard'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo disable the Kubernetes Dashboard on an existing cluster, run the following\ncommand:\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003czone\u003e \\\n    --update-addons=KubernetesDashboard=DISABLED\n","test_info":["Using Command Line:\nTo disable the Kubernetes Dashboard on an existing cluster, run the following\ncommand:\n  gcloud container clusters update \u003ccluster_name\u003e --zone \u003czone\u003e \\\n    --update-addons=KubernetesDashboard=DISABLED\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.10.2","test_desc":"Ensure that Alpha clusters are not used for production workloads (Automated)","audit":"gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.enableKubernetesAlpha'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nUpon creating a new cluster\n  gcloud container clusters create [CLUSTER_NAME] \\\n    --zone [COMPUTE_ZONE]\n\nDo not use the --enable-kubernetes-alpha argument.\n","test_info":["Using Command Line:\nUpon creating a new cluster\n  gcloud container clusters create [CLUSTER_NAME] \\\n    --zone [COMPUTE_ZONE]\n\nDo not use the --enable-kubernetes-alpha argument.\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.10.3","test_desc":"Consider GKE Sandbox for running untrusted workloads (Manual)","audit":"gcloud container node-pools describe $NODE_POOL --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.config.sandboxConfig'\n","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Using Command Line:\nTo enable GKE Sandbox on an existing cluster, a new Node pool must be created,\nwhich can be done using:\n  gcloud container node-pools create \u003cnode_pool_name\u003e --zone \u003ccompute-zone\u003e \\\n    --cluster \u003ccluster_name\u003e --image-type=cos_containerd --sandbox=\"type=gvisor\"\n","test_info":["Using Command Line:\nTo enable GKE Sandbox on an existing cluster, a new Node pool must be created,\nwhich can be done using:\n  gcloud container node-pools create \u003cnode_pool_name\u003e --zone \u003ccompute-zone\u003e \\\n    --cluster \u003ccluster_name\u003e --image-type=cos_containerd --sandbox=\"type=gvisor\"\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"},{"test_number":"5.10.5","test_desc":"Enable Security Posture (Manual)","audit":"gcloud container clusters --location describe","AuditEnv":"","AuditConfig":"","type":"manual","remediation":"Enable security posture via the UI, gCloud or API.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/protect-workload-configuration\n","test_info":["Enable security posture via the UI, gCloud or API.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/protect-workload-configuration\n"],"status":"WARN","actual_value":"","scored":false,"IsMultiple":false,"expected_result":"","reason":"Test marked as a manual test"}]}],"total_pass":0,"total_fail":1,"total_warn":32,"total_info":0}],"Totals":{"total_pass":15,"total_fail":2,"total_warn":40,"total_info":0}}
